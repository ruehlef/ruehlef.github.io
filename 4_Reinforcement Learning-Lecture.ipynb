{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37244cd7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Today's topic, Reinforcement Learning, is closed to what you might have traditionally meant by \"artificial intelligence,\" which maybe isn't a word that you would necessarily apply to a CNN that labels pictures of galaxies (though maybe you would.) When we thing of artificial intelligence, we think of a computer that can learn to play chess, or a robot that can learn to walk. Reinforcement learning is the field of machine learning that deals with these kinds of problems.\n",
    "\n",
    "The essential ideas of RL are:\n",
    "- an *agent* is exploring an *environment.*\n",
    "- at any given time, the agent perceives a *state* of the environment, $s\\in S$.\n",
    "- the agent can take *actions* that change the state of the environment, $a\\in A$.\n",
    "- but how does the agent choose? via a *policy function* $\\pi:S\\rightarrow A$. The policy might be deterministic, or it might be stochastic, an $s$-dependent probability density on $A$.\n",
    "- by following the policy, the agent generates a trajectory through state space, that terminates either at some fixed timeout or when the agent reaches a *terminal state,* characterized by an end goal like checkmate.\n",
    "- at each time step, the agent receives a *reward* $r_t$ that depends on the state, which accumulate into a *return*\n",
    "    $$G_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}$$\n",
    "    where $\\gamma\\in[0,1]$ is a *discount factor* that determines how much the agent cares about the future.\n",
    "- the expected return given a state is the *value function* a.k.a. *state value function* of a state, \n",
    "        $$v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s],$$\n",
    "  and the expected return given a state-action pair is the *action value function*,\n",
    "        $$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a].$$\n",
    "\n",
    "The goal of RL is to find a policy that maximizes the expected return. Some algorithms do this by estimating the value function or action-value function, and then using it to find the optimal policy. These are called *value-based* algorithms. Others directly estimate the optimal policy. These are called *policy-based* methods, or policy gradients.\n",
    "\n",
    "As a litmus test for whether RL is right for your problem, you should ask whether it naturally admits a description in terms of an agent exploring an environment, and taking particular actions to do so. Also necessary is a notion of what is \"good\" or \"bad\" in the environment, to set up your reward. If the state space is small enough, the problem can be solved by going through all the states brute-force. However, in cases with exponentially large state spaces, such as Chess or Go or String Theory, we can't go through all the states and instead use techniques that approximate the value function or policy. In practice, those techniques are neural networks, and the associated RL techniques are called *deep reinforcement learning.*\n",
    "\n",
    "For more on RL, see:\n",
    "- [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html), a famous early textbook.\n",
    "- [David Silver's course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), a more modern take. The videos were my entry point into RL.\n",
    "- [AlphaZero](https://www.nature.com/articles/nature24270), a 2017 breakthrough in which RL achieves superhuman gameplay in Go *without human knowledge*, i.e. only via knowledge of the game and self-play. It was extended to Chess and Shogi in 2018. See the arXiv article here: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815).\n",
    "- My friends and I introduced RL into string theory in [Branes with Brains](https://arxiv.org/abs/1903.11616). We have also used it to [find unknots](https://arxiv.org/abs/2010.16263) and [ribbons](https://arxiv.org/abs/2304.09304), the latter in connection with the smooth 4d Poincar√© conjecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d245a1c",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "In this class we'll study a famous game in RL called *Gridworld*, see. e.g. Sutton and Barto for more. It's a simple game that is easy to understand, but still has some interesting features. The game is played on a grid, and the agent can move up, down, left, or right. The agent starts in a random position, and the goal is to reach the goal state, which is chosen via the policy.\n",
    "The game ends when the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36fc3c",
   "metadata": {},
   "source": [
    "### Defining Gridworld\n",
    "This module defines the Gridworld game environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69aabb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gridworld environment\n",
    "# Original code downloaded from https://gist.github.com/kfeeeeee/e81aeeff0516cfd7645c8e99cd4fa315#file-gridworld-py\n",
    "# Modifications made by Fabian Ruehle, who completely change the game (rules, rewards, goal, colors)\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "\n",
    "# This represents an object in the game: worker, pitfall, exit\n",
    "class GameOb:\n",
    "    def __init__(self, name, reward, coordinates, size, RGBA):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.channel = RGBA\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "# The gridworld environment\n",
    "class GameEnv:\n",
    "    def __init__(self):\n",
    "        # initialization of the world\n",
    "        self.sizeX = 9\n",
    "        self.sizeY = 9\n",
    "        self.num_pits = 7\n",
    "        self.state = ()  # A state in gridworld is just the (x,y) coordinate pair of the worker\n",
    "        self.objects = []\n",
    "        self.initial_x = 0\n",
    "        self.initial_y = 0\n",
    "        self.gave_up = False\n",
    "        self.fell = False\n",
    "\n",
    "        # We want the worker to solve the maze as fast as possible without falling into the pits:\n",
    "        # *)   -1 for each step (penalty to solve it quickly)\n",
    "        # *)  -50 for each pitfall (penalty for falling into the pit)\n",
    "        # *) +100 for finding the exit (reward for solving the maze)\n",
    "        # *)   -2 for running into a wall / not moving at all\n",
    "        self.step_penalty = -1.\n",
    "        self.pitfall_penalty = -50.\n",
    "        self.exit_reward = 100.\n",
    "        self.no_move_penalty = -2.\n",
    "\n",
    "        # Actions in gridworld: move up, down, left, right\n",
    "        self.action_space = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "        # keep track of the total number of steps and the steps that were taken in the game\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "\n",
    "        # maximal n umber of steps before we give up solving the maze\n",
    "        self.max_steps = 1000\n",
    "\n",
    "        # plotting the maze\n",
    "        plt.ioff()  # there is currently a bug for Mac users which requires turning this off\n",
    "        self.world_canvas = plt.figure(\"Maze\")\n",
    "        self.world_canvas.suptitle('Blue: Worker, Red: Pitfalls, Green: Exit')\n",
    "        self.im = None\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # initialize and plot the world\n",
    "        self.world = self.initialize_world()\n",
    "        self.snapshot_world = self.initialize_world()\n",
    "\n",
    "    # initialize a new random world\n",
    "    def initialize_world(self):\n",
    "        self.objects = []\n",
    "\n",
    "        # 1.) The first parameter is the name of the object\n",
    "        # 2.) The second parameter is the reward / penalty:\n",
    "        # 3.) The third parameter is the position of the object in the world\n",
    "        # 4.) Ignore the other parameters, they are just used for drawing the world (box sizes and color)\n",
    "\n",
    "        # fix position of exit and worker\n",
    "        # maze_exit = GameOb('exit', self.exit_reward, self.new_position(), 1, [0, 1, 0, 1])\n",
    "        maze_exit = GameOb('exit', self.exit_reward, [4, 4], 1, [0, 1, 0, 1])\n",
    "        self.objects.append(maze_exit)\n",
    "        # worker = GameOb('worker', None, self.new_position(), 1, [0, 0, 1, 1])\n",
    "        worker = GameOb('worker', None, [0, 0], 1, [0, 0, 1, 1])\n",
    "        self.objects.append(worker)\n",
    "        for i in range(self.num_pits):  # add pitfalls\n",
    "            pitfall = GameOb('pitfall', self.pitfall_penalty, self.new_position(), 1, [1, 0, 0, 1])\n",
    "            self.objects.append(pitfall)\n",
    "\n",
    "        # store the initial (x,y) coordinates for a reset\n",
    "        self.initial_x = worker.x\n",
    "        self.initial_y = worker.y\n",
    "\n",
    "        # show the world\n",
    "        world = self.render_world()\n",
    "\n",
    "        # initialize/ reset the variables\n",
    "        self.reset()\n",
    "\n",
    "        # plot the world\n",
    "        plt.ioff()\n",
    "        self.im = plt.imshow(world, interpolation=\"nearest\")\n",
    "\n",
    "        return world\n",
    "\n",
    "    # reset the world to its initial configuration, ignore this\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "        self.gave_up = False\n",
    "        self.fell = False\n",
    "        self.state = (self.initial_x, self.initial_y)\n",
    "        # np.random.seed(random.randint(0, 100000))\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                obj.x = self.initial_x\n",
    "                obj.y = self.initial_y\n",
    "                break\n",
    "\n",
    "    # move through the world\n",
    "    # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "    def move_worker(self, direction):\n",
    "\n",
    "        # identify the worker amongst the gridworld objects\n",
    "        worker = None\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                worker = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "\n",
    "        worker_x = worker.x\n",
    "        worker_y = worker.y\n",
    "\n",
    "        # overall reward/penalty\n",
    "        reward = self.step_penalty  # penalize each move\n",
    "\n",
    "        # update the position of the worker in gridworld (move if possible)\n",
    "        if direction == 0 and worker.y >= 1:\n",
    "            worker.y -= 1\n",
    "        if direction == 1 and worker.y <= self.sizeY - 2:\n",
    "            worker.y += 1\n",
    "        if direction == 2 and worker.x >= 1:\n",
    "            worker.x -= 1\n",
    "        if direction == 3 and worker.x <= self.sizeX - 2:\n",
    "            worker.x += 1\n",
    "\n",
    "        # move was illegal\n",
    "        if worker.x == worker_x and worker.y == worker_y:\n",
    "            reward = self.no_move_penalty\n",
    "\n",
    "        # update to new position\n",
    "        for i in range(len(self.objects)):\n",
    "            if self.objects[i].name == 'worker':\n",
    "                self.objects[i] = worker\n",
    "                break\n",
    "\n",
    "        # check whether new field is a special field (exit/pitfall) and compute reward/penalty\n",
    "        is_maze_solved = False\n",
    "        for other in others:\n",
    "            if worker.x == other.x and worker.y == other.y:  # the worker ran into an object\n",
    "                if other.name == \"exit\":  # the object was an exit\n",
    "                    is_maze_solved = True\n",
    "                    # print \"I found the exit,yay!\"\n",
    "                    reward = other.reward\n",
    "                    break  # we can exit the loop here since we can only run into one object\n",
    "                elif other.name == \"pitfall\":  # the object was a pitfall\n",
    "                    is_maze_solved = False\n",
    "                    reward = other.reward\n",
    "                    self.fell = True\n",
    "                    break   # we can exit the loop here since we can only run into one object\n",
    "\n",
    "        return reward, is_maze_solved\n",
    "\n",
    "    # perform the step, collect the reward, check whether you have reached the exit\n",
    "    def step(self, action, update_view=True):\n",
    "\n",
    "        # collect the reward/punishment for the field the worker ends up in and check whether the exit was reached\n",
    "        reward, done = self.move_worker(action)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.steps_taken.append(action)\n",
    "\n",
    "        # give up\n",
    "        if self.steps >= self.max_steps and not done:\n",
    "            done = True\n",
    "            self.gave_up = True\n",
    "\n",
    "        # fell into pit\n",
    "        if self.fell:\n",
    "            done = True\n",
    "\n",
    "        # this just updates the graphic output of the world\n",
    "        if update_view:\n",
    "            # world = self.render_world()\n",
    "            # plt.imshow(world, interpolation=\"nearest\")\n",
    "            self.im.set_array(self.render_world())\n",
    "            plt.draw()\n",
    "\n",
    "        # return the new state, the penalty/reward for the move and whether gridworld is solved/given up on\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    # get the current state\n",
    "    def get_state(self):\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                return obj.x, obj.y\n",
    "\n",
    "    # check whether an action is possible, i.e. whether a wall is blocking the way\n",
    "    def is_possible_action(self, action):\n",
    "        is_possible = False\n",
    "        if action == 0 and self.state[1] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 1 and self.state[1] <= self.sizeY - 2:\n",
    "            is_possible = True\n",
    "        if action == 2 and self.state[0] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 3 and self.state[0] <= self.sizeX - 2:\n",
    "            is_possible = True\n",
    "\n",
    "        return is_possible\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # ignore the code from here on, it just draws the world and represents the objects in the game.                    #\n",
    "    ####################################################################################################################\n",
    "    def close_world_display(self):\n",
    "        plt.close(\"Gridworld\")\n",
    "\n",
    "    def new_position(self):\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        current_position = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in current_position:\n",
    "                current_position.append((objectA.x, objectA.y))\n",
    "        for pos in current_position:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def render_world(self):\n",
    "        a = np.zeros([self.sizeY + 2, self.sizeX + 2, 4])\n",
    "        a[0:, 0, 3] = 1  # left wall\n",
    "        a[0, 0:, 3] = 1  # top wall\n",
    "        a[0:, self.sizeX + 1, 3] = 1  # right wall\n",
    "        a[self.sizeY + 1, 0:, 3] = 1  # bottom wall\n",
    "        a[1:-1, 1:-1, :] = 1\n",
    "        for item in self.objects:\n",
    "            if a[item.y + 1, item.x + 1, 0] == 1 and a[item.y + 1, item.x + 1, 1] == 1 and a[item.y + 1, item.x + 1, 2] == 1:  # is completely white\n",
    "                for i in range(len(item.channel)):\n",
    "                    a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] = item.channel[i]\n",
    "            else:  # other object on the field, overlay worker with pitfalls / exit\n",
    "                for i in range(len(item.channel)):\n",
    "                    if a[item.y + 1, item.x + 1, i] == 0:\n",
    "                        a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] += item.channel[i]\n",
    "        a = np.array(Image.fromarray(np.uint8(a * 255)).resize((84, 84), Image.NEAREST))\n",
    "        return a\n",
    "\n",
    "    def plot_world(self, title=\"\"):\n",
    "        plt.imshow(self.render_world())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a235a95-0dac-420f-81d0-2272155224fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "# get the best action. If several actions are equally good, choose a random one\n",
    "def get_best_action(dct):\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# randomize the action in 100*eps percent of the cases\n",
    "def random_action(action, action_space, eps=0.3):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Animate the steps taken, ignore this\n",
    "step_counter = 0\n",
    "explore_step = None\n",
    "def animate_steps(agent, window_title, fig_title=\"\"):\n",
    "    plt.ioff()\n",
    "    fig = plt.figure(window_title)\n",
    "    fig.suptitle(fig_title)\n",
    "    mySteps = agent.steps_taken\n",
    "    agent.reset()\n",
    "    step_counter = 0\n",
    "    explore_step = mySteps[step_counter]\n",
    "    im = plt.imshow(agent.render_world(), animated=True)\n",
    "\n",
    "    def update_fig(*args):\n",
    "        nonlocal explore_step, step_counter\n",
    "        if step_counter < len(mySteps):\n",
    "            explore_step = mySteps[step_counter]\n",
    "        else:\n",
    "            step_counter = 0\n",
    "            explore_step = mySteps[step_counter]\n",
    "            agent.reset()\n",
    "        agent.step(explore_step, False)\n",
    "        step_counter += 1\n",
    "        im.set_array(agent.render_world())\n",
    "        plt.draw()\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_fig, interval=150, blit=True, frames=len(mySteps)-1, repeat=True)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(window_title)\n",
    "    # plt.show()\n",
    "    ani.save(\"./\" + window_title + \".gif\", writer=animation.PillowWriter(fps=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97372061-a3d9-4be1-911c-b241d37f2305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = GameEnv()\n",
    "agent.plot_world(\"world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e76f",
   "metadata": {},
   "source": [
    "Now we have the agent explore Gridworld. Since the state space is small, we can explore the entire environment and find optimal policies. We will do so using an algorithm called *SARSA*. The basic idea is to estimate the action-value function $q_\\pi(s,a)$, and then use it to find the optimal policy. The algorithm is as follows:\n",
    "- Initialize $Q(s,a)$ arbitrarily.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efadae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Part 1: Play the game once to see an untrained agent at work\n",
    "########################################################################################################################\n",
    "agent.reset()\n",
    "agent.close_world_display()\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# To start the algorithm, we need any action, so we pick one randomly until we find a valid action which we perform\n",
    "\n",
    "# loop until done (i.e. solved the maze or gave up)\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 2: Show the exploration route taken by the untrained worker\n",
    "########################################################################################################################\n",
    "result = \"\"\n",
    "if not agent.gave_up and reward > 0:\n",
    "    result = \"I solved gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"Sorry, I had to give up after \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the first game\n",
    "print(\"Watch my exploration route...\")\n",
    "animate_steps(agent, \"Gridworld exploration untrained worker\", result)\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 3: Play the game 10,000 times to learn the best solution strategy\n",
    "########################################################################################################################\n",
    "print(\"Now let me train for a while, I enjoyed the game so much!\")\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "# The code is essentially identical to the one used above, but now carried out 10,000 times\n",
    "\n",
    "\n",
    "print(\"Ok, I am done practicing.\")\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 4: Show the exploration route taken by the trained worker\n",
    "########################################################################################################################\n",
    "\n",
    "# Navigate the maze using the best steps as learned by the agent\n",
    "\n",
    "\n",
    "if not agent.gave_up and reward > 0:\n",
    "    result = \"I can now solve Gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"I haven't learned solving Gridworld in \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the trained worker\n",
    "animate_steps(agent, \"Gridworld exploration trained worker\", result)\n",
    "\n",
    "print(\"Thanks for playing! Bye.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b433a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as idp\n",
    "idp.Image(url='./Gridworld exploration untrained worker.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idp.Image(url='./Gridworld exploration trained worker.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5456ee4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates the integration and functionality of a Gridworld game environment using Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "venv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
