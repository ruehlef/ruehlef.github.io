<!DOCTYPE html>
<html lang="en">
<head>
	<title>Fabian Ruehle - Research</title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="shortcut icon" type="image/x-icon" href="icons/favicon.ico">
	<link rel="stylesheet" href="./css/mainstyle.css">
	<link rel="stylesheet" href="./css/collapse.css">
	<script id="MathJax-script" async src="./script/mathjax/tex-chtml.js"></script>
</head>
<body>

	<div class="header">
	  <h1>Fabian Ruehle - Research</h1>
	</div>

	<div class="navbar large">
	  <a href="./index.html">Home</a>
	  <a href="./interests.html">Interests</a>
	  <a href="./seminars.html">Seminars</a>
	  <a href="./talks.html">Presentations</a>
	  <a href="./teaching.html">Teaching</a>
	  <a href="./research.html" class="active">Research</a>
	  <a href="./cv.html">CV</a>
	</div>
	<div class="navbar small">
	  <div class="menu-toggle"><img src="./icons/menu/menu_icon.png" alt="menu"/></div>
	  <a href="./index.html">Home</a>
	  <a href="./interests.html">Interests</a>
	  <a href="./seminars.html">Seminars</a>
	  <a href="./talks.html">Presentations</a>
	  <a href="./teaching.html">Teaching</a>
	  <a href="./research.html" class="active">Research</a>
	  <a href="./cv.html">CV</a>
	</div>

	<div class="row">
	  <div class="side">
		<div class="sectiontitle">About Me</div>
		<table style="margin: auto;"><tr><td>
			<div class="photo">
			  <img src="./img/ruehle.jpg" alt="Fabian Ruehle"/>
			</div>
			<div class="info">
				Fabian Ruehle<br/>
				Assistant Professor<br/>
				<a href="https://cos.northeastern.edu/physics/" target="_blank">Physics</a> and <a href="https://cos.northeastern.edu/mathematics/" target="_blank">Mathematics</a><br/>
				<a href="https://iaifi.org" target="_blank">IAIFI Senior Investigator</a>
				<hr/>
				<a href="https://goo.gl/maps/eYy7QUFap6b1ALMw7" target="_blank">
				Northeastern University<br/>
				Dana Research Center<br/>
				Office 223<br/>
				Boston, MA 02115
				</a><br/>
				Phone: +1 617-373-8109
				<hr/>
			</div>
			<div class="media">
			  <a href="mailto:f.ruehle@northeastern.edu" style="margin-right: 5px;"><img src="./icons/mail/mail.png" width=32 alt="email"/></a>
			  <a href="https://inspirehep.net/authors/1061173?ui-citation-summary=true" target="_blank" style="margin-right: 5px;"><img src="./icons/inspire/inspire.jpg" width=32 alt="inspire"/></a>
			  <a href="https://scholar.google.com/citations?hl=en&user=Sk5U1goAAAAJ" target="_blank" style="margin-right: 5px;"><img src="./icons/google-scholar/google-scholar.png" width=32 alt="google scholar"/></a>
			  <a href="https://github.com/ruehlef" target="_blank" style="margin-right: 5px;"><img src="./icons/GitHub-Mark/github.png" width=32 alt="github"/></a>
			</div>
	    </td></tr></table>
	  </div>
	  
	  <div class="main">
	    <noscript>Please enable Javascript to display this website properly.<br/></noscript>

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Recent developments at the intersection of AI and theoretical physics</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2505.07956" target="_blank">[arxiv:2505.07956]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2505.07956_1.png" alt="Symbolic Regression flow chart" width=240 /></p><p><img src="./img/news/2505.07956_2.png" alt="Example KAN for symbolic regression on 2 variables" width=240 /></p></div>
				<p class="newsparagraph">
				Symbolic regression is a process where, given some input and output data, one tries to infer an analytic function that can be used to obtain the output from the given input. This is crucial for interpreting black box functions such as neural networks. The challenge is that there is an endless possibility of combining or concatenating functions, and existing methods usually try to fit a pre-defined function basis.<br/>
				We suggest to use LLMs like ChatGPT to guess an ansatz for the function from a graph of the data, which we then successively refine based on how well the guess fits the function. This does not require specifying a function basis; the LLM will now all functions it encountered in its training set. However, this approach works only for unary functions, i.e., functions in one variable (since higher functions cannot be plotted easily).<br/>
				In order to apply the idea to multi-variate problems, we leverage <a href="https://arxiv.org/abs/2404.19756" target="_blank">Kolmogorov-Arnold Networks</a> (KANs), which allow to write any function as sums of unary functions, i.e., we use symbolic regression on each edge of the KAN, build the final expression by summation and concatenation, and simplify it using traditional techniques as well as LLMs. See figures for the workflow and an example that identifies \(f(x,y)=xy\). The code is available on <a href="https://github.com/harveyThomas4692/llmlex" target="_blank"">GitHub</a>.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Recent developments at the intersection of AI and theoretical physics</div>
					<div style="font-size:14px;"><a href="https://www.tandfonline.com/doi/full/10.1080/00107514.2025.2498817" target="_blank">[Contemporary Physics]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/ContemporaryPhysics.png" alt="Diffusion Model" width=240 /></p></div>
				<p class="newsparagraph">
				We discuss recent developments in artificial intelligence that are specifically geared towards application and scientific discovery in theoretical and mathematical physics. Along the way, we point out other phenomena at the intersection of Physics and AI, including chaos theory, phase transitions, spin glasses, symmetries, classical and statistical mechanics (see figure), and quantum theory.<br/>
				The target audience are undergraduate or graduate students in physics or mathematics with no background in AI or machine learning, and provides the theoretical foundation for application of AI in these research areas as well as how physics can serve to better understand neural networks.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Learning Topological Invariance</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2504.12390" target="_blank">[arxiv:2504.12390]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2504.12390_1.png" alt="Contrastive Learning" width=240 /></p><p><img src="./img/news/2504.12390_2.png" alt="Some sampled Knots" width=240 /></p></div>
				<p class="newsparagraph">
				Topology is an equivalence relation where two spaces are equivalent if they differ by certain geometric deformations. We explore techniques to train a Neural Network to learn topological invariance, i.e., maps where an entire equivalence class is mapped to the same object, which we choose to be a vector in \(\mathbb{R}^N\), see first figure. By definition, this vector then corresponds to a topological invariant. We use contrastive learning as well as generative techniques with a transformer to achieve this.<br/>
				Throughout the paper, we illustrate the process using knot theory. Once we have learned a topologically invariant map from multiple equivalence classes of knots with multiple representatives each to vectors in \(\mathbb{R}^N\) with a NN, we develop a technique to extract the topological invariant(s) that are computed by the NN: We train smaller NNs (the "student") to reproduce the output of the trained NNs (the "teacher"), but the student gets handed known topological invariants instead of topological spaces. The extent to which the student can learn to reproduce the teacher's invariants from its input quantifies how much of this topological invariance is used by the teacher. By subtracting off the students' embeddings from the teachers' embeddings, we are left with an "irreducible" piece that corresponds to a new invariant.<br/>
				Finally, we use these techniques to address the Jones Unknot Conjecture, which states that a topological knot invariant called the Jones polynomial is trivial iff the knot from which it is computed is the unknot. We use different sampling techniques to explore the learned embeddings around the unknot, which produce new knots with simple Jones polynomials (see second figure), but we fail to find a conterexample, i.e., a knot that has trivial Jones polynomial but is not the unknot.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">On the Learnability of Knot Invariants: Representation, Predictability, and Neural Similarity</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2502.12243" target="_blank">[arxiv:2502.12243]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2502.12243_1.png" alt="Prediction of Invariants" width=240 /></p><p><img src="./img/news/2502.12243_2.png" alt="Neural Similarity" width=240 /></p></div>
				<p class="newsparagraph">
				Deciding whether two knots are can be smoothly deformed into one another is a central topic of low-dimensional topology. To this end many knot invariants have been proposed based on knot diagrams, geometric, and homological data. Moreover, many different encodings of knots have been used in the literature. In this paper, we ask which knot representations are most beneficial for a neural network to learn which invariants. Conversely, we also study which knot invariants can be learned to which degree.<br/>
				We find that the diagrammatic invariants as well as the geometric invariants  based on hyperbolic geometry can be learned almost perfectly for most knot representations, while the homological knot invariants are more complicated to lear. In particular, no network (we tried MLPs, CNNs, and transformers) managed to learn predicting the Arf invariant for any representation of the knot. Across invariants, representing knots as braid words allows for the most accurate predictions, while predictions based on representing the knot as a polygonal curve was worst (see top figure), even for invariants that should be computable more efficiently in this representation.<br/>
				Finally, we propose two neural similarity measures for distinct but mathematically related invariants. One is based on a cosine similarity score of the gradient saliency vectors of different networks, and the other is based on studying jointly misclassified knots across networks for different knot invariants (see bottom figure).
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Interpretable Machine Learning for Kronecker Coefficients</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2502.11774" target="_blank">[arxiv:2502.11774]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2502.11774.png" alt="KAN Decision Boundary" width=240 /></p></div>
				<p class="newsparagraph">
				We study Kronecker coefficients which appear in the decomposition of representations of the symmetric group \(S_N\). These combinatorial objects are interesting from a computational complexity perspective: While determining whether the Littlewood–Richardson coefficients for \(GL_N(\mathbb{C})\) are vanishing is in P, the problem is NP hard for Kronecker coefficients.<br/> 
				By using interpretable machine learning techniques, such as PCA, gradient saliency analysis of small neural networks, and Kolmogorov-Arnold Networks (KAN), we identify a sufficient criterion for when the Kronecker coefficients are non-vanishing. The KAN analysis reveals that around 80 percent of the coefficients can be classified correctly if we take the mean of the b-loadings (defined in <a href="https://arxiv.org/abs/2310.17906">[2310.17906]</a>) as a decision boundary, see figure.<br/> 
				Finally, we set up a transformer network that can predict whether the Kronecker coefficients are vanishing with above 99 percent accuracy. This could be taken as an indication that while the problem is NP-hard, the vanishing of most Kronecker coefficients is actually easy to decide, and that there exists a very accurate algorithm for the problem.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2412.15184" target="_blank">[arxiv:2412.15184]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td>
				<p class="newsparagraph">
				The datasets commonly used to train and test AI-based mathematical assistants—such as large language models—have significant limitations. Most notably, they tend to focus on relatively simple mathematics, rarely going beyond lower undergraduate-level content. They also rely on rigid evaluation methods, like binary correctness checks, which are insufficient for assessing AI capabilities in proof-based reasoning.<br/> 
				We analyze these shortcomings in detail and argue that improving AI-powered mathematical assistants requires a fundamental shift in how mathematical datasets are designed and evaluated. Instead of focusing only on theorem-proof pairs, datasets should capture the broader workflow of mathematical research, i.e., the step-by-step, often field-specific reasoning processes that lead to new discoveries. These workflows are crucial for proof discovery but are largely absent from existing datasets.<br/>
				To address this, we highlight the importance of motivated proof, a concept introduced by Pólya in 1949, which provides a more structured way to guide AI through the reasoning process. We also propose a new standard for documenting mathematical datasets: math datasheets. These datasheets, inspired by general-purpose dataset documentation, include a targeted questionnaire that helps dataset creators identify potential weaknesses in their datasets while making it easier for users to assess their suitability for training and evaluating mathematical AI assistants.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">A Twist on Heterotic Little String Duality</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2411.05313" target="_blank">[arxiv:2411.05313]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2411.05313.png" alt="Duality Network" width=240 /></p></div>
				<p class="newsparagraph">
				In this work, we extend the known web of T-dualities among heterotic NS5-brane theories with eight supercharges by introducing new types of twists. These twists involve reordering certain gauge and flavor symmetries, as well as permuting tensor multiplets when compactifying on a circle. To systematically compare different T-dual theories, we identify key properties that remain unchanged under duality—such as the structure of the Coulomb branch, higher-group symmetries, and the ranks of flavor symmetry groups.<br/>
				By analyzing these invariants, we establish precise connections between geometric singularities of the compactification space, the number of NS5-branes, and the corresponding flavor symmetry factors. The twisted theories arise naturally from M-theory compactifications on special types of Calabi-Yau threefolds, which have a torus fiber but no section. This perspective allows us to demonstrate dualities between twisted and untwisted theories using M/F-theory duality, where different fibrations of the same geometry correspond to distinct field theories (see figure).<br/>
				We also construct new 5D theories, including a novel CHL-like twisted theory where the two M9-branes are identified. By analyzing their invariants, we also determine their dual descriptions. 
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    		
		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Attractors, Geodesics, and the Geometry of Moduli Spaces</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2408.00830" target="_blank">[arxiv:2408.00830]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2408.00830.png" alt="Split attractor flows" width=240 /></p></div>
				<p class="newsparagraph">
				We study a series of recent conjectures using the attractor mechanism. First, we observed in <a href='https://arxiv.org/abs/2304.00027' target='_blank'>[2304.00027]</a> a relation between attractor points and degeneracies in the spectrum of the scalar Laplacian. A recent conjecture by Etheridge <a href='https://arxiv.org/abs/2311.18693' target='_blank'>[2311.18693]</a> states that the eigenvalues of the scalar Laplacian with respect to the Calabi-Yau metric are eigenfunctions of the Laplacian on the moduli space with respect to the Weil-Petersson metric. We show that this is true for \(T^2\), but fails for some BPS states in \(T^4\).<br/> 
				Another interesting conjecture by Raman and Vafa in <a href='https://arxiv.org/abs/2405.11611' target='_blank'>[2405.11611]</a> states that the marked moduli space of a Calabi-Yau manifold is contractible, which is related to the statement that geodesics in the marked moduli space are unique. Since attractor flows are geodesics, we discuss potential failure modes when attractor flows split along walls of marginal stability (see figure). Another way geodesic uniqueness could break down is when there are flop walls in the K\"ahler moduli space that separate isomorphic Calabi-Yaus. We discuss both cases, which are counter-examples in the moduli space but not in the marked moduli space. 
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">A Heterotic Kähler Gravity and the Distance Conjecture</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2406.04393" target="_blank">[arxiv:2406.04393]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2406.04393.png" alt="New Complexes" width=240 /></p></div>
				<p class="newsparagraph">
				We study deformations of the cubic superpotential in heterotic string theory. Since the theory is at most quadratic in the moduli fields, we can integrate them out and obtain a new holomorphic theory, which shares similarities with Kodaira-Spencer gravity and holomorphic Chern-Simons theory. In the topological theory, a certain combination of H-flux and the holomorphic top form take on the role of the metric, and we define quantities such as the Hodge star \(\star_H\) with respect to this object. We also introduce a new symplectic cohomology theory where the complex structure deformation plays the role of the symplectic form (see image). <br/>
				We spend some time to gauge-fix the action using the Batalin–Vilkovisky mechanism. We show that the resulting kinetic operator \(\partial\star_H\partial\) is elliptic, which leads to a discrete, gapped eigenvalue spectrum for the Laplacian. We then show that the gap closes exponentially fast as the complex structure moduli are sent to infinity, which verifies that the theory satisfies the swampland distance conjecture.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Bounds and Dualities of Type II Little String Theories</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2405.03877" target="_blank">[arxiv:2405.03877]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2405.03877-IILSTs.png" alt="Some Type II LST quivers" width=240 /></p></div>
				<p class="newsparagraph">
				In this paper, we continue our study of 6D LSTs, but this time we look at Type II LSTs. In contrast to the Heterotic LSTs of <a href="https://arxiv.org/abs/2311.02168" target="_blank">[2311.02168]</a>, Type II LSTs have a much more constrained flavor sector. In fact, we use a mix of anomaly inflow arguments and unitarity constraints from the worldsheet theory of the little string, as well as some group theory, to argue that the rank of the flavor symmetry can be at most 2.<br/>
				While the flavor sector of Type II LSTs is less rich, the defect group structure is much richer. We argue that the 1-form and 2-form symmetries get exchanged under the T-duality.<br/> 
				Finally, we construct a large landscape of Type II LSTs both from the bottom up and contrast it with a geometric top-down construction (see Figure for a few example theories). The latter uses a decompactification of a Schoen-like Calabi-Yau manifold, which makes the double elliptic fibration structure and with it T-duality manifest.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
		
		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">KAN: Kolmogorov-Arnold Networks</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2404.19756" target="_blank">[arxiv:2404.19756]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2404.19756-kan.gif" alt="Freezing D4" width=240 /></p></div>
				<p class="newsparagraph">
				We introduce a new type of neural network, called KAN, as an alternative to standard feed-forward neural networks. They are based on Kolmogorov-Arnold representation theory, which means for our purposes that we can represent any function by a collection of piece-wise cubic polynomials (so-called cubic splines). In contrast to standard NNs, this means that the function basis is fixed (to be piece-wise cubic) rather than learned from weights plus activation functions, so we only have to tune the parameters that control the individual cubic splines. While this is more expensive than a standard NN, KANs typically can get away with much fewer parameters, thus offsetting the cost.  Moreover, their piecewise nature prevents catastrophic forgetting which plagues NNs. Fewer parameters also mean that KANs are much more interpretable, see the animation for an example.<br/>
				We have tested them on a collection of tasks, from solving differential equations, to problems in knot theory, and Anderson localization in condensed matter theory. They performed on par or better than much larger NNs in these tasks. For the problems in knot theory, interpretatbility meant that we could recover (trivial and non-trivial) relations among knot invariants. We provide a code library with a KAN implementation on <a href="https://github.com/KindXiaoming/pykan" target="_blank">GitHub</a> if you want to try them out.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">The Frozen Phase of Heterotic F-theory Duality</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2404.02191" target="_blank">[arxiv:2404.02191]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2404.02191-freezing.png" alt="Freezing D4" width=240 /></p></div>
				<p class="newsparagraph">
				String Theory (more precisely F-Theory) has been used to construct consistent supergravity theories in six dimensions. In addition to the classical theories with ADE gauge groups, the landscape contains more exotic theories as well. One set of such theories is known as theories with frozen singularities. Essentially, there is a discrete flux which prevents the singularities from becoming smooth. This phenomenon is related to so-called O7+ branes in Type IIB string theories, models without vector structure in Heterotic SO(32) string theory, and the CHL string in Heterotic E8 theory. While F-theory allows to study all of them via duality, the construction remains complicated. For example, for F-theory, a model with an O7- brane and 4 D7 branes on top (which is a standard SO(8) gauge theory) looks equivalent to (meaning this has the same monodromy as) a model with a single O7+ brane.<br/> 
				We give a recipe that allows to construct consistent frozen models in F-Theory, infer their gauge and particle content (see Figure for an example), and suggest new consistency conditions for such compactification. For the heterotic duals, we relate freezing to a folding of ADE Dynknin diagram. This is particularly beautiful for the exceptional groups, where the folding is related to certain platonic solids.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">On classical de Sitter solutions and parametric control</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2403.07065" target="_blank">[arxiv:2403.07065]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2403.07065.png" alt="Basis choices" width=240 /></p></div>
				<p class="newsparagraph">
				The question of whether well-controlled de Sitter solutions exist, and if so whether they can be obtained from string theory, remains an open and much-discussed topic. In this paper, we study string theory compactifications on a specific class of 6D manifolds called solv-manifolds. Loosely speaking, these are Lie groups (and hence, they are also manifolds) with an underlying lattice that makes the manifold compact. Topologically, they are torus fibrations, and mathematicians have classified all possibilities. The advantage of using these constructions over other Calabi-Yau constructions is that the geometry can be described very explicitly in terms of the Lie algebra structure constants.<br/>
				We focus on a specific type IIB setup with D5 and O5 brane stacks. Finding solutions for the sets of equations that encode all consistency conditions for a classical, controlled, dS solution (such as the equations of motion for the fields, compactness constraints, demanding a positive CC, large radii, small couplings, etc.) requires solving a mixed integer non-linear programming problem. These are non-linear system of equalities and inequalities where some variables take integer values and others take values over the real numbers. Such problems are notoriously difficult to solve. What is more, working out the consistency conditions required a change of basis (see Figure on the right) that makes the problem numerically even more ill-conditioned. We nevertheless discuss multiple avenues to attack this problem and present some solutions. We also discuss a discrete scaling parameter in our solutions (that allows to scale up radii and fluxes) and explain how the solution exploits a loophole in no-go theorems against parametrically controlled de Sitter solutions. Finally, we briefly comment on phenomenological applications, such as the dark dimension and quintessence scenarios.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    
	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Rigor with Machine Learning - from Field Theory to the Poincare Conjecture</div>
					<div style="font-size:14px;"><a href="https://www.nature.com/articles/s42254-024-00709-0" target="_blank">[Nature Reviews Physics]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td>
				<p class="newsparagraph">
				Machine learning techniques are notoriously difficult to interpret and often produce stochastic results. For these reasons, fields like pure mathematics and formal theoretical physics are "late adopters" of these techniques. In this invited contribution to Nature Reviews, we describe how we can harness the power of ML despite these challenges.<br/> 
				One avenue is to use ML for experimental mathematics, which can lead to generation of mathematical conjectures that are proven subsequently by a human. Another avenue is to use a subfield of ML called reinforcement learning, which can produce verifiable  results to decision problems. Finally, one can use exact results from ML theory to obtain tractable results. We illustrate these ideas using recent examples from the literature.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">T-Duality and Flavor Symmetries in Little String Theories</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2311.02168" target="_blank">[arxiv:2311.02168]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2311.02168-e6.png" alt="E6 folding" width=240 /></p></div>
				<p class="newsparagraph">
				We study six-dimensional little string theories (LSTs) as well as their circle compactifications to 5D. It is known that different 6D theories can become the same theory in 5D under T-duality, and it is interesting to ask which 6D quantities are preserved under this duality. Some quantities like the Coulomb branch dimension and the two-group structure constants have been argued to be invariants under the duality, and we argue that the rank of the flavor group of the 6D theories is preserved as well.<br>
				We establish that via geometric engineering of the 6D theories. Along the way, we construct novel LSTs and discuss several exotic properties. The appearance of non-simply laced flavor groups, for example, is linked to a specific geometric construction that leads to folding of the flavor Dynkin diagram (see Figure). We also construct self-dual models and models with multiple T-duals.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->


		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Metric Flows with Neural Networks</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2310.19870" target="_blank">[arxiv:2310.19870]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2310.19870-flows.png" alt="Flows" width=240 /></p><p><img src="./img/news/2310.19870-generalization.png" alt="Performance" width=240 /></p></div>
				<p class="newsparagraph">
				We consider metric flows, which means that the metric on some space changes dynamically following its equations of motion. Mathematicians have studied such flows extensively, i.e., the metric changes proportional to the gradient of some action or energy functional. The perhaps best-known flow is Perelman's Ricci flow, which he used to prove the 3D Poincare conjecture, but other flows exist such as the Mabuchi flow or the Calabi-flow for Kähler metrics.<br/>
				When Neural Networks (NNs) learn Calabi-Yau metrics, they also flow to the target metric following gradient descend of an underlying loss functional. While these flows are quite complicated for "usual" NNs, they surprisingly become very simple for "infinitely large" neural networks.<br/>
				We discuss how flows like the Ricci flow can be obtained as a special case of NN metric flows, which require a specific architecture and using the Perelman energy functional for the NN loss functional (see figure for a summary of these flows). In practice, we find that infinitely wide NNs (or other kernel methods we study) do not outperform finite NNs, hinting at the importance of the NN being able to learn a useful embedding of the CY (the inputs to a NN are called features in ML, and in our case the inputs are points on the CY). By definition, feature learning requires the NN to dynamically adapt to the input features, while infinitely wide NNs (or other static kernels) use the same static, predefined function during the entire training process. Such methods fail to generalize (far) beyond the training set and would require very dense samples of the CY space (see second figure).
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
		<!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Searching for ribbons with machine learning</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2304.09304" target="_blank">[arxiv:2304.09304]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2304.09304-ribbon.png" alt="Ribbon" width=240 /></p><p><img src="./img/news/2304.09304-improvement.png" alt="Performance" width=240 /></p></div>
				<p class="newsparagraph">
				A knot is a closed (knotted) string embedded in a three-dimensional space (a three-sphere, say). As such, it is the boundary of some surface in the three-sphere, see the first figure. If this surface is a disk, the knot is called slice. A closely related object is a ribbon knot: roughly, a knot is ribbon if you can cut it open at two positions and put a band in between the cuts such that you obtain a collection of circles. Every ribbon knot is slice, but it is an open conjecture whether every slice knot is ribbon.<br/>
				There are topological obstructions that can tell you when a knot is not slice, but no known ones that would tell you when it is slice. Knowing whether a knot is slice is not just important for the aforementioned slice-ribbon-conjecture, but can also be used to construct potential counter-examples to the smooth Poincare conjecture in four dimensions (SPC4).<br/>
				While this sounds like a binary classification problem that could be tackled with Neural Networks, we actually do not phrase it as such. The reason is that the output of the Neural Network would just give you a certain probability with which it thinks the input was slice, which is not good enough. Instead, we use Reinforcement Learning and Bayesian optimization to obtain a sequence of moves that manipulates the knot until it is manifestly a collection of circles and hence ribbon (and hence slice). By recording the actions, we have a verifiable truth certificate that a given knot actually was slice - established with 100% certainty from ML.<br/> 
				We rule out thousands of proposed counter-examples to SPC4 (leaving only 2) and establish ribbonness for new knots. The code can operate on very large knots and often establishes ribbonness in seconds (see second figure). It is available on my <a href="https://github.com/ruehlef/ribbon">Github page</a>. 
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    
	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Level Crossings, Attractor Points and Complex Multiplication</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2304.00027" target="_blank">[arxiv:2304.00027]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2304.00027-gmoduli.png" alt="moduli space metric" width=240 /></p><p><img src="./img/news/2304.00027-attractorxing.png" alt="Level Crossings" width=240 /></p></div>
				<p class="newsparagraph">
				In a <a href="https://arxiv.org/abs/2103.07472" target="_blank">previous paper</a>, Anthony and I observed that the eigenmodes of the scalar Laplacian run up and down as a function of the shape (complex structure) of the compact extra dimensions of string theory (Calabi-Yau (CY) manifolds). In particular, this means that the eigenvalues cross at some points. This begs the question: <b>What is special about these crossing points?</b><br/>
				Looking at the equation that defines the CY, the CY metric, or the moduli space metric (see first figure), nothing special seems to be happening when we insert the complex structure values where crossings occur. So we study a simpler CY, the torus. In that case, we actually know everything analytically, and we find that the crossing points can be associated with enhanced symmetries of the torus, which are known as complex multiplication points in algebraic number theory. Fascinatingly, Moore related these points to attractor points, which are specific values of the complex structure related to supersymmetric black holes. We thus speculate that level crossings might be related to these attractor points. If correct, this would be very interesting, since the Black Holes enjoy "protection" by supersymmetry (they are BPS), while the Laplace spectrum is not known to be BPS.<br/>
				We check the relation between Complex Multiplication, attractor points, and eigenvalue crossings numerically by computing the CY metric and from it the spectrum on the one hand, and solving the attractor flow equations on the other. Our results are consistent with the two appearing at the same point (see second figure). We also discuss how the quality of our numerical spectrum is impacted by the different approximations we need to make along the way.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Symmetries of Calabi-Yau Prepotentials with Isomorphic Flops</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2210.09369" target="_blank">[arxiv:2210.09369]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2210.09369-triangle.png" alt="triangle group" width=240 /></p><p><img src="./img/news/2210.09369-cones.png" alt="Restricted cones" width=240 /></p></div>
				<p class="newsparagraph">
				In his seminal paper, Coxeter wrote in 1934: "[Take] three vertical mirrors, erected on the sides of an equilateral triangle so as to form a prism, open at the top. A candle placed within this prism gives rise to an unlimited number of images." This was generalized shortly thereafter by Vinberg to the case where the mirrors do not necessarily  form a prism, but more generally a polyhedron or cone. Among cones string theorists deeply care about are Kahler cones and their duals, called Mori cones. We noticed recently that topological transitions across Kahler cone walls of Calabi-Yau (CY) manifolds exist which lead to new CYs that are isomorphic to the old one. The Kahler cone of such a space after the transition is obtained by reflection along the corresponding Kahler cone wall. It is easy to construct spaces with multiple such "reflecting Kahler cone walls", giving rise to infinitely many equivalent (isomorphic) CYs, whose (infinitely many) individual Kahler cone glues to one big Kahler cone, and we explain how to describe them in terms of Coxeter groups. For 3 Kahler cone walls, this is illustrated (on the hyperbolic Poincare Disk) in the figure on the left. <br>
				Under mirror symmetry, this big cone, consisting of infinitely many Kahler cones, becomes the complex structure moduli space of one single CY, and it is interesting to ask what this infinite reflection symmetry does to this space: it implies that curve classes inside the manifold necessarily need to carry the same number of curves (counted by Gopakumar-Vafa invariants) if they are mapped onto each other by reflections.<br>
				This leads to very strong vanishing statements: Only curve classes the lie inside the dual of the extended Kahler cone can be non-vanishing (the green dots inside the green cone in the bottom picture). Moreover, this reflection can lead to very special behavior of instanton corrections: after factoring out GV invariants that agree since they correspond to curve classes that are mapped onto one another, the part that is left over arranges to form special functions, among them modular Jacobi Theta functions. We also link the appearance of these modular functions to an elliptic fibration structure of the CY.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Flops for Complete Intersection Calabi-Yau Threefolds</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2112.12106" target="_blank">[arxiv:2112.12106]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2112.12106-1.png" alt="branched double cover" width=240 /></p><p><img src="./img/news/2112.12106-2.png" alt="GTP formula" width=240 /></p></div>
				<p class="newsparagraph">
				We discuss flops in projective Complete Intersection Calabi-Yau manifolds. We explain that there are two different types of flops, whose presence and type can be read off from the GLSM charges. Of course, this can also be used to engineer manifolds that exhibit a certain flop type.
				<br/>
				<b>Case 1:</b> A GLSM U(1) charge contains only 0's and 1's. This signals the presence of flops to non-isomorphic manifolds.<br/>
				<b>Case 2:</b> A GLSM U(1) charge contains a single 2, followed by 0's and 1's. This signals the presence of flops to isomorphic manifolds.<br/>
				We explain in both cases how to describe the manifold as well as the singular manifold that sits in between two flops. These require moving away from projective CICYs to more general toric models, determinantal varieties (for case 1), or more complicated manifolds (in the second case). As shown in the figure, these are small resolutions of branched double covers of the "base".
				<br/>
				We also explain how to describe the flopped divisors (and in fact the isomorphism of the Picard groups) under both types of flops. We also explain how to get the number of contracted curves in the flop from either comparing the Euler characteristic or by using the Giambelli-Thom-Porteous formula (see bottom figure for an example).
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Recent Developments in Line Bundle Cohomology and Applications to String Phenomenology</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2112.12107" target="_blank">[arxiv:2112.12107]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2112.12107.jpg" alt="Extended Kahler cone" width=240 /></p></div>
				<p class="newsparagraph">
				This is a conference proceeding to the <a href="https://sites.google.com/view/cim-dialogues/home" target="_blank">Nankai Symposium on Mathematical Dialogues: In celebration of S.S.Chern's 110th anniversary</a>.
				We review progress in deriving closed-form expressions for line bundle cohomologies and discuss applications to string phenomenology. We also review results from the papers <a href="https://arxiv.org/abs/2104.03325" target="_blank">[2104.03325]</a>, <a href="https://arxiv.org/abs/2108.10323" target="_blank">[2108.10323]</a>, and <a href="https://arxiv.org/abs/2112.12106" target="_blank">[2112.12106]</a> on the structure and properties of the extended Kahler cone of CICY manifolds.
				<br/>
				The figure illustrates the structure of the (convex) extended Kahler cone in the case of infinitely many flops. The extended Kahler cone actually has infinitely many subcones, which become, however, smaller and smaller (just in the original coordinate system; their diameter, as measured with the moduli space metric, actually stays the same), such that they cannot be seen in the image.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Learning Size and Shape of Calabi-Yau Spaces</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2111.01436" target="_blank">[arxiv:2111.01436]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2111.01436-1.png" alt="Sigma loss" width=240 /></p><p><img src="./img/news/2111.01436-2.png" alt="Ricci loss" width=240 /></p></div>
				<p class="newsparagraph">
				For the <a href="https://ml4physicalsciences.github.io/2021/" target="_blank">NeurIPS ML and the Physical Sciences workshop</a>, we presented our library <a href="https://github.com/pythoncymetric/cymetric" target="_blank">cymetric</a> to compute Calabi-Yau metrics for any projective complete intersection Calabi-Yau or toric Kreuzer-Skarke Calabi-Yau.
				We vastly extend to scope by implementing general point sampling methods on the CY with known distributions for any toric or projective ambient space (i.e. for any \(h^{1,1}\)). We also introduce a new architecture, the "Phi Model", which allows to find the Ricci flat metric in a fixed Kahler class.
				<br/>
				The code is written in TensorFlow, but we also provide an API for SageMath and for Mathematica. The latter allows to train a NN to approximate the CY metric with just a few lines of Mathematica code and does not require any knowledge of Python. 
				<br/>
				We demonstrate the code for a few example Calabi-Yau manifolds (see top figure for how the Monge-Ampere equation gets solved to higher and higher accuracy during training for various CY metric ansatze). The code can compute many quantities related to CY metrics, including the Ricci tensor, the Ricci scalar, and the Kahler potential (in the calse of the Phi Model). The change in the Ricci scalar as a function of the change in the Monge-Ampere equation is plotted in the bottom figure. We see that once the Monge-Ampere equation is solved exactly, the metric is Ricci-flat; this had to happen, since the Calabi-Yau metric is unique, and it is very nice to see this reproduced. Interestingly, the origin is approached along a line.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Geodesics in the extended Kähler cone of Calabi-Yau threefolds</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2108.10323" target="_blank">[arxiv:2108.10323]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2108.10323-case1.png" alt="Case 1" width=240 /></p><p><img src="./img/news/2108.10323-case2.png" alt="Case 2" width=240 /></p><p><img src="./img/news/2108.10323-case3.png" alt="Case 3" width=240 /></p><p><img src="./img/news/2108.10323-bounce.png" alt="Bounce" width=240 /></p></div>
				<p class="newsparagraph">
				We continue our study of the moduli space of a Calabi-Yau (CY) and the topology changes that can occur. We find that as one traverses the moduli space, three things can happen:
				<br/>
				<b>Case 1:</b> A curve collapses but the volume of divisors and the CY stays finite (see first picture). This indicates a "flop wall". One can go through this singularity and emerge on the other side in another geometry where the collapsed curve is replaced by another curve.<br/>
				<b>Case 2:</b> A divisor collapses, but the CY volume stays finite (see second picture). This indicates a somewhat mysterious boundary which we call a Zariski wall. The CY volume stays finite, but one nevertheless does not have a geometric description beyond this wall.<br/>
				<b>Case 3:</b> The volume of the whole CY collapses (see third picture). This indicates the end of the effective cone. One would need a GLSM or some other description to make sense of string theory beyond this wall.<br/>
				<br/>
				For CYs with 2 Kahler parameters in M-Theory, we can solve the geodesics equation of motion analytically (case 3 features every trigonometric function in the book...) in the vector moduli multiplet space of the theory, which essentially means solving the equations along curves of constant overall volume. We find:<br/>
				<b>Case 1:</b> These are always at finite geodesic distance.<br/>
				<b>Case 3:</b> These are always at infinite geodesic distance (this is obvious, since the volume of the CY shrinks in case c, but we are moving along curves of constant volume).<br/>
				<b>Case 2:</b> This is the strangest case. If we wanted to probe what’s beyond the Zariski wall and happily follow our geodesic, we find that the geodesic “bounces off” the Zariski wall and is reflected back into the geometric regime we came from (see fourth picture). And that bounce happens at finite distance. Moreover, this bounce is not benign: the closer we get to the wall, the more the geodesic motion accelerates, reaching infinite speed at the wall. So it crashes into the wall with infinite speed and is re-emitted into the opposite direction, also with infinite speed. Of course our theory is invalidated before reaching infinite speed, and it is suggested that an additional SU(2) theory appears at such walls. Presumably, if we could correctly include the SU(2) vector multiplet, the singularity would be cured, but we don’t attempt this in this paper.<br/>
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
		
	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">String Pheno 2021</div>
					<div style="font-size:14px;"><a href="https://indico.cern.ch/e/sp2021" target="_blank">Conference Homepage</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/sp2021.png" alt="stringpheno21" width=240 /></p></div>
				<p class="newsparagraph">
				In the week of July 12th to July 16th 2021, Northeastern is hosting the 20th installment of the anual <a href="https://indico.cern.ch/e/sp2021" target="_blank">String Phenomenology conference</a>, the largest conference in this field. Due to the pandemic, the conference takes place virtually. 
				It covers a range of topics, including 
				<br/>
				&bull;&nbsp;&nbsp;&nbsp;Swampland and quantum gravity conjectures<br/>
			    &bull;&nbsp;&nbsp;&nbsp;Machine learning and AI techniques in string theory<br/>
				&bull;&nbsp;&nbsp;&nbsp;Formal and mathematical aspects of string compactifications<br/>
				&bull;&nbsp;&nbsp;&nbsp;String model building in particle physics and cosmology
				<br/>
				If you are interested, just <a href="https://indico.cern.ch/event/1034944/registrations/" target="_blank">register</a> to receive Zoom links to the talks.
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">A blood atlas of COVID-19 defines hallmarks of disease severity and specificity</div>
					<div style="font-size:14px;"><a href="https://www.medrxiv.org/content/10.1101/2021.05.11.21256877v1" target="_blank">[medrxiv]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/Combat-consensus.png" alt="consensus clustering" width=240 /></p><p><img src="./img/news/COMBAT-feature_score.png" alt="feature scores" width=240 /></p></div>
				<p class="newsparagraph">
				In a collaboration with biologists, clinicians, and mathematicians, we studied various aspects of COVID19. I used ML to answer two questions:<br/>
				1.) In collaboration with two colleagues from genomic medicine and a radiologist, and one from respiratory immunology, we studied which (combination of) clinical parameters best describes the status of hospitalized covid patients (mild, severe, critical). The analysis was carried out using unsupervised ML tools (k-means consensus clustering). The classification of the health status assigned by medical doctors matched nicely onto the clinical variables, cf. top figure, which shows three clusters for mild, severe, critical identified by the unsupervised ML algorithm. This gave us confidence that we are using the right measure to classify patients.<br/>
				2.) Given the above classification in terms of clinical variables, I used different laboratory data (gained from RNA sequencing, Luminex analysis, Mass Spectrometry, ...) to to identify proteins or genes that lead to severe desease progression in covid patients. For this I first computed principal components and scored them (using feature permutation scoring) based on 20 ML classification algorithms. I then took the best-performing Prinicpal Components and reran the analysis for the proteins/genes that loaded these Components the strongest. A summary of the genes/proteins that were most important in distinguishing mild, severe, and critical patients are given in the bootom figure.<br/>
				Overall, I think this is a great example that demonstrates the power for transfer of knowledge across the field based on the common language of ML.
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->

	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Swampland Conjectures and Infinite Flop Chains</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2104.03325" target="_blank">[arxiv:2104.03325]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2104.03325-oscillation.png" alt="equivalent flops" width=240 /></p><p><img src="./img/news/2104.03325-sym_flop_geod.png" alt="inequivalent flops" width=240 /></p></div>
				<p class="newsparagraph">
				As you walk through the moduli space of a Calabi-Yau (CY), the CY can change its topology. Very common topological transitions are flops. Flops can lead to new or to equivalent CYs, and there are even cases where infinitely many flops occur. This begs the question:
				<br/>
				<i>What happens to the Swampland Distance Conjecture (SDC), i.e. which tower of states becomes light?</i> 
				<br/>
				Clearly, for flops to equivalent CYs, the entire spectrum is identical, yet the geodesic distance between equivalent points in different cones is non-zero. In fact, it can be arbitrarily large in the infinite flop case, see top figure (while the figure looks like the cones get smaller and smaller, the moduli space metric behaves such that the distance between the walls of each cone is the same; indeed, we can identify the symmetry that maps all cones onto one another). We argue that in such cases there is a remnant discrete gauged symmetry, inherited from the 11D Poincare symmetry of M-theory, which identifies all K&auml;hler cones. Consequently, the shortest geodesic connecting two equivalent points corresponds to not moving at all. 
				<br/>
				So what about the case with inequivalent flops? If there are finitely many, there is a divisor at the boundary of the K&auml;hler cone (denoted by \(D_L\) and \(D_R\) in the bottom figure). Approaching a wall then means that the winding modes on this divisor become light, providing the tower of light states predicted by the SDC.
				<br/>
				This resolution is, however, not available if there exist infinitely many flops to inequivalent CYs. In these cases, there is no divisor at the "end" of the K&auml;hler cone, and hence no associated tower can become light. In contrast to infinitely many equivalent CYs, we haven't seen such cases yet, and indeed a conjecture by Kawamata and Morrison says that they cannot occur. Conversely, the SDC applied to infinitely many inequivalent flops implies the Kawamata-Morrison conjecture. I think it's fun to use physics in this way to explore properties of CYs!
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
	    
	    <!--###################################################################################################################################################################################################-->
		<div class="newsdiv">
		  <table class="newstable">
			<tr>
				<td colspan="2">
					<div class="newstitle">Moduli-dependent KK towers and the Swampland Distance Conjecture on the Quintic</div>
					<div style="font-size:14px;"><a href="https://arxiv.org/abs/2103.07472" target="_blank">[arxiv:2103.07472]</a></div>
				</td>
			</tr>
			<tr class="newsrow">
			<td><div class="newsimg"><p><img src="./img/news/2103.07472-metric.png" alt="metric" width=240 /></p><p><img src="./img/news/2103.07472-spectrum.png" alt="spectrum" width=240 /></p></div>
				<p class="newsparagraph">
				Take for example the Klein-Gordon equation \(g^{ab}\partial_a\partial_b \varphi = m^2\varphi\). We see that the massive spectrum depends on the metric. We can compute moduli-dependent Calabi-Yau metrics, and hence we can compute (moduli-dependent) massive KK states on Calabi-Yau manifolds. The swampland distance conjecture tells us that this KK tower should become exponentially light when moving \(O(1)\) distance (in Planck units) in moduli space (see top image). In this paper, we compute geodesics in moduli space to see what it means to move \(O(1)\) in terms of the parameters that describe the CY.<br/>
				For K&auml;hler moduli, it means that the numerical value of the K&auml;hler parameters \(t_i\) changes indeed by \(O(1)\). For complex structure moduli \(\psi\), it means that the numerical value changes by \(O(1000)\).  This is clear from mirror symmetry, since \(t=\log\psi\).<br/>
				We then compute the massive eigenmodes of the CY Laplacian as we vary the moduli and find that states become light as \(m_\text{KK} \sim e^{-0.45 d}\), where \(d\) is the distance in moduli space, so one can move 2-3 Planck units before the KK tower comes down one e-fold. I think this is the first and only way of determining this constant (for finite distance). This confirms the SDC on this manifold, but also shows that moving a few Planck units can be feasible. <br/>
				Finally, we observe a curious pattern in the eigenmodes: They exhibit level crossing, and eigenmodes that transform in larger irreps of the CY symmetry group become heavier, while those in small irreps become lighter (see bottom image). We have no understanding of this phenomenon. If you do, please tell us!
				</p>
			</td>
			</tr>
		  </table>
		  <p class="readMore">more...</p>
	    </div><hr/>	
	    <!--###################################################################################################################################################################################################-->
		
	  </div>
	</div>
	
    <script src="./script/collapse.js"></script>
	<div class="footer">
	&#169; Fabian Ruehle  
	</div>

</body>
</html>